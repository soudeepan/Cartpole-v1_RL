{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24de4c2e-09f9-4591-aaab-2bd996dfad51",
   "metadata": {},
   "source": [
    "# About the Environment (Cartpole-v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75c22dd-85ef-4943-b3d0-4eab455e386d",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "This environment corresponds to the version of the cart-pole problem described by Barto, Sutton, and Anderson in “Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problem”. A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum is placed upright on the cart and the goal is to balance the pole by applying forces in the left and right direction on the cart."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a21abce-999a-41c9-919a-75c85d229b5f",
   "metadata": {},
   "source": [
    "## Action Space\n",
    "\n",
    "The action is a ndarray with shape (1,) which can take values {0, 1} indicating the direction of the fixed force the cart is pushed with.\n",
    "\n",
    "- 0: Push cart to the left\n",
    "- 1: Push cart to the right\n",
    "\n",
    "Note: The velocity that is reduced or increased by the applied force is not fixed and it depends on the angle the pole is pointing. The center of gravity of the pole varies the amount of energy needed to move the cart underneath it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1630d91f-4bfe-421d-9bfe-8be3bd39e39f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Observation Space\r\n",
    "\r\n",
    "The observation is a ndarray with shape (4,) with the values corresponding to the following positions and velocities:\r\n",
    "\r\n",
    "| Num | Observation           | Min                  | Max                  |\r\n",
    "|-----|-----------------------|----------------------|----------------------|\r\n",
    "| 0   | Cart Position         | -4.8                 | 4.8                  |\r\n",
    "| 1   | Cart Velocity         | -Inf                 | Inf                  |\r\n",
    "| 2   | Pole Angle            | ~ -0.418 rad (-24°) | ~ 0.418 rad (24°)   |\r\n",
    "| 3   | Pole Angular Velocity | -Inf                 | Inf                  |\r\n",
    "\r\n",
    "**Note:** While the ranges above denote the possible values for the observation space of each element, it is not reflective of the allowed values of the state space in an unterminated episode. Particularly:\r\n",
    "\r\n",
    "- The cart x-position (index 0) can take values between (-4.8, 4.8), but the episode terminates if the cart leaves the (-2.4, 2.4) range.\r\n",
    "- The pole angle can be observed between (-0.418, 0.418) radians (or ±24°), but the episode terminates if the pole angle is not in the range (-0.2095, 0.2095) (or ±12°).\r\n",
    "ewards   |\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb83300-ed96-4e07-909e-c2d42141b20f",
   "metadata": {},
   "source": [
    "## Rewards\n",
    "\n",
    "Since the goal is to keep the pole upright for as long as possible, a reward of +1 for every step taken, including the termination step, is allotted. The threshold for rewards is 500 for v1 and 200 for v0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2967e56-66f2-4530-995f-d5c5b6eac546",
   "metadata": {},
   "source": [
    "## Starting State\n",
    "All observations are assigned a uniformly random value in (-0.05, 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c45b9bb-6331-48f2-aec3-306a001216be",
   "metadata": {},
   "source": [
    "## Episode End\n",
    "The episode ends if any one of the following occurs:\n",
    "\n",
    "1. Termination: Pole Angle is greater than ±12°\n",
    "2. Termination: Cart Position is greater than ±2.4 (center of the cart reaches the edge of the display)\n",
    "3. Truncation: Episode length is greater than 500 (200 for v0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26149e76-6066-44dd-ba48-b4eadbfcf3bc",
   "metadata": {},
   "source": [
    "# Loading Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18353d39-c937-42dc-8795-7a561ef74f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import gym \n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fd6a2e-b5cf-47eb-b977-b21eb0c72da2",
   "metadata": {},
   "source": [
    "# loading Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "82c2f275-f836-4aca-840a-c98c138191ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_name = \"CartPole-v1\"\n",
    "env = gym.make(environment_name, render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf131db-0902-4a7b-9b41-9d32046e3019",
   "metadata": {},
   "source": [
    "# Testing with random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "385e16d4-47f4-4e33-8397-ec2541e6b834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.02001622, -0.03263483,  0.03480381, -0.04523245], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "026fd093-a78d-47a7-a87c-f78f77818faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45f17e44-2bfb-4360-b755-d45bc64196f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Soudeepan\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:15.0\n",
      "Episode:2 Score:86.0\n",
      "Episode:3 Score:29.0\n",
      "Episode:4 Score:28.0\n",
      "Episode:5 Score:15.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 5\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, _, info, = env.step(action)\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30372cb9-890e-4a14-b7cf-94072fc7b84c",
   "metadata": {},
   "source": [
    "# Setup Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "793ca395-41c4-4bbb-b4d6-b3ba6260b6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Soudeepan\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "save_path = os.path.join('Training', 'Saved Models')\n",
    "log_path = os.path.join('Training', 'Logs')\n",
    "stop_callback = StopTrainingOnRewardThreshold(reward_threshold=490, verbose=1)\n",
    "eval_callback = EvalCallback(env, \n",
    "                             callback_on_new_best=stop_callback, \n",
    "                             eval_freq=10000, \n",
    "                             best_model_save_path=save_path, \n",
    "                             verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60cc1ec-a2a6-410f-b852-bde3f9e18070",
   "metadata": {},
   "source": [
    "# Training RL Model (PPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "951bbf26-b1e9-4e09-a356-4da16e18974a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "env = DummyVecEnv([lambda: env])\n",
    "model = PPO('MlpPolicy', env, verbose = 1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04e8954d-3724-4c03-8bf5-eb5a36f20fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\Logs\\PPO_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Soudeepan\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 46   |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 43   |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 45          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 89          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007474929 |\n",
      "|    clip_fraction        | 0.0737      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.687      |\n",
      "|    explained_variance   | -0.00524    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.1        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0106     |\n",
      "|    value_loss           | 55          |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 44           |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 136          |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076351054 |\n",
      "|    clip_fraction        | 0.0445       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.671       |\n",
      "|    explained_variance   | 0.0955       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 18.5         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.0126      |\n",
      "|    value_loss           | 45.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 44           |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 182          |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0107297655 |\n",
      "|    clip_fraction        | 0.0857       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.639       |\n",
      "|    explained_variance   | 0.223        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 32.3         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.0195      |\n",
      "|    value_loss           | 59.7         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Soudeepan\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=10000, episode_reward=250.40 +/- 105.07\n",
      "Episode length: 250.40 +/- 105.07\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 250          |\n",
      "|    mean_reward          | 250          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 10000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074244626 |\n",
      "|    clip_fraction        | 0.0787       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.617       |\n",
      "|    explained_variance   | 0.208        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 24.7         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0198      |\n",
      "|    value_loss           | 75.7         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 40    |\n",
      "|    iterations      | 5     |\n",
      "|    time_elapsed    | 253   |\n",
      "|    total_timesteps | 10240 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 41           |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 299          |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071011083 |\n",
      "|    clip_fraction        | 0.0396       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.601       |\n",
      "|    explained_variance   | 0.11         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 21           |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.0118      |\n",
      "|    value_loss           | 77.4         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 41          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 343         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006529997 |\n",
      "|    clip_fraction        | 0.0469      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.595      |\n",
      "|    explained_variance   | 0.349       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 25.5        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0135     |\n",
      "|    value_loss           | 65.1        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 42           |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 388          |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042718025 |\n",
      "|    clip_fraction        | 0.0297       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.584       |\n",
      "|    explained_variance   | 0.675        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 18.7         |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00686     |\n",
      "|    value_loss           | 55.6         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 42           |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 433          |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028071795 |\n",
      "|    clip_fraction        | 0.0167       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.59        |\n",
      "|    explained_variance   | 0.684        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 26.5         |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00541     |\n",
      "|    value_loss           | 49.2         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=485.20 +/- 29.60\n",
      "Episode length: 485.20 +/- 29.60\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 485          |\n",
      "|    mean_reward          | 485          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 20000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075188302 |\n",
      "|    clip_fraction        | 0.0983       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.567       |\n",
      "|    explained_variance   | 0.861        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.22         |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.0117      |\n",
      "|    value_loss           | 32.7         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 38    |\n",
      "|    iterations      | 10    |\n",
      "|    time_elapsed    | 528   |\n",
      "|    total_timesteps | 20480 |\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x1a97b8d3590>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=20000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e367d8-4c29-4be9-a59a-988da91aae10",
   "metadata": {},
   "source": [
    "# Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d5dc7e6-02c3-4e98-8919-82f3d11461f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_path = os.path.join(\"Training\",\"Saved_models\",\"PPO_cartpole_model\")\n",
    "model.save(ppo_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93a32b1-135d-47b5-b333-1359178f0b5d",
   "metadata": {},
   "source": [
    "# Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9f9ef21-0726-4a3b-86fc-21ee34cab801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Soudeepan\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ppo_path = os.path.join(\"Training\",\"Saved_models\",\"PPO_cartpole_model\")\n",
    "model = PPO.load(ppo_path,env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b849aaf6-d01f-4617-83ae-00c27f877294",
   "metadata": {},
   "source": [
    "# Testing and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "357552a3-07c0-4e7d-bf66-a6763c28f885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(496.0, 12.0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=10, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e93df8c3-cb5f-437a-b32c-5a12ac70d7a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(1, dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict((0,0,0,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b27af49-aaa0-4ae6-967e-9714a366f622",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You have passed a tuple to the predict() function instead of a Numpy array or a Dict. You are probably mixing Gym API with SB3 VecEnv API: `obs, info = env.reset()` (Gym) vs `obs = vec_env.reset()` (SB3 VecEnv). See related issue https://github.com/DLR-RM/stable-baselines3/issues/1694 and documentation for more information: https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m      8\u001b[0m     env\u001b[38;5;241m.\u001b[39mrender()\n\u001b[1;32m----> 9\u001b[0m     action,_ \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(obs)\n\u001b[0;32m     10\u001b[0m     obs, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     11\u001b[0m     score\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mreward\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\base_class.py:556\u001b[0m, in \u001b[0;36mBaseAlgorithm.predict\u001b[1;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[0;32m    537\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    538\u001b[0m     observation: Union[np\u001b[38;5;241m.\u001b[39mndarray, Dict[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    541\u001b[0m     deterministic: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    542\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, Optional[Tuple[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]]]:\n\u001b[0;32m    543\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    544\u001b[0m \u001b[38;5;124;03m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;124;03m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[38;5;124;03m        (used in recurrent policies)\u001b[39;00m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 556\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mpredict(observation, state, episode_start, deterministic)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:357\u001b[0m, in \u001b[0;36mBasePolicy.predict\u001b[1;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;66;03m# Check for common mistake that the user does not mix Gym/VecEnv API\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;66;03m# Tuple obs are not supported by SB3, so we can safely do that check\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(observation) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m--> 357\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    358\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have passed a tuple to the predict() function instead of a Numpy array or a Dict. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    359\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are probably mixing Gym API with SB3 VecEnv API: `obs, info = env.reset()` (Gym) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    360\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvs `obs = vec_env.reset()` (SB3 VecEnv). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    361\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee related issue https://github.com/DLR-RM/stable-baselines3/issues/1694 \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    362\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand documentation for more information: https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    363\u001b[0m     )\n\u001b[0;32m    365\u001b[0m obs_tensor, vectorized_env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_to_tensor(observation)\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[1;31mValueError\u001b[0m: You have passed a tuple to the predict() function instead of a Numpy array or a Dict. You are probably mixing Gym API with SB3 VecEnv API: `obs, info = env.reset()` (Gym) vs `obs = vec_env.reset()` (SB3 VecEnv). See related issue https://github.com/DLR-RM/stable-baselines3/issues/1694 and documentation for more information: https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api"
     ]
    }
   ],
   "source": [
    "episodes = 5\n",
    "for episode in range(1, episodes+1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action,_ = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29d5e8e-d6dd-44f9-864b-e030641b004d",
   "metadata": {},
   "source": [
    "# Viewing Logs in Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7792baf9-e766-48da-bd74-9d62006df5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_log_path = os.path.join(log_path, 'PPO_5')\n",
    "training_log_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da01f0aa-3ecc-40e8-bc3a-d8e8bdb5ea28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do it in command matrics\n",
    "!tensorboard --logdir={training_log_path} "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
